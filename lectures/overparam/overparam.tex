\documentclass{article}

\def\ParSkip{} 
\input{../../common/ryantibs}

\title{Overparametrized Regression: Ridgeless Interpolation \\ \smallskip
\large Advanced Topics in Statistical Learning, Spring 2023 \\ \smallskip
Ryan Tibshirani }
\date{}

\begin{document}
\maketitle
\RaggedRight
\vspace{-50pt}

\section{Introduction}

Current practice in machine learning suggests that it ``works'' to design neural
networks that are massively overparametrized, and train them without explicit
regularization until they interpolate the training data and thus have zero
training error. Surprisingly, these models can still have good prediction error.     

You might say: so what? We should regularize and the models will stop
interpolating and they'll also perform better! However, in some overparametrized  
settings, it can actually be that tuning over the regularization strength (in an 
estimator like ridge regression) can suggest that a vanshing amount of
regularization is optimal. 

There has been some influential and thought-provoking experimental work in
support of these phenomena. See Figure \ref{fig:experiments} for a few
examples. How can we understand this through the lens of statistical theory?
We can start by understanding what happens in linear models. Even though linear
models are ``as old as statistics itself'', we may learn something new.    

\begin{figure}[p]
\centering
\hspace{-20pt}
\includegraphics[height=0.39\textwidth]{belkin1.png}
\includegraphics[height=0.39\textwidth]{belkin2.png} 
\bigskip\medskip \\
\includegraphics[width=0.69\textwidth]{liang.png} 
\bigskip \\
\includegraphics[width=0.98\textwidth]{kobak.png}
\caption{\it Experiments demonstrating the motivating points raised in the
  introduction. Interpolating estimators (with zero training error) can still   
  have good prediction error, as shown in the top row, credit:
  \citet{belkin2019reconciling}. Tuning over regularization levels can 
  suggest that vanishing regularization is optimal even in high 
  dimensions, as shown in the middle row, credit: \citet{liang2020just}, and
  bottom row, credit: \citet{kobak2020optimal}. The behavior of the prediction
  risk in the top row is dubbed ``double descent''.}   
\label{fig:experiments}
\end{figure}

\subsection{What's new here?}

Statisticians have been interested in high-dimensional models for a long
time. So what's new here? Don't the terms ``high-dimensional'' (in use for
several decades) and ``overparametrized'' (popularized recently) refer the same
thing?    

In a sense, the answer is both ``yes'' and ``no''. While traditional
high-dimensional analyses (like those we studied for the lasso) and newer
overparametrized analyses are clearly very related, they are also different in
some key ways. In the study of overparametrized models:
\begin{itemize}
\item we focus exclusively on out-of-sample prediction error---unlike many 
  traditional high-dimensional regression analyses which focus on in-sample
  prediction error (and treat out-of-sample prediction error as somewhat of an 
  afterthought);  
\item we care about how the risk landscape behaves as we vary the regularization  
  strength, and particularly what happens for vanishing explicit
  regularization---unlike many traditional analyses which study regimes where
  optimal performance is given by strong explicit regularization.   
\end{itemize}
There are arguably other differences, but those are two of the most salient ones 
for our discussion.

\subsection{Ridgeless least squares}

Given a response vector $Y \in \R^n$ and predictor matrix $X \in \R^{n \times
  d}$, as usual, we consider the \emph{minimum $\ell_2$ norm least squares} 
(or simply min-norm least squares) estimator,
\begin{equation}
\label{eq:ls_min}
\hbeta = (X^\T X)^+ X^\T Y,
\end{equation}
where $(X^\T X)^+$ denotes the Moore-Penrose pseudoinverse of $X^\T X$.
Equivalently, we can write this as
\[
\hbeta = \argmin \Big\{ \|\beta\|_2 : \beta \; \text{minimizes} \; \|Y - X
\beta\|_2^2 \Big\},   
\]
which explains its name.  

An alternative name for \eqref{eq:ls_min} is the ``ridgeless'' least squares
estimator. This is because it can be written as the limit of the ridge estimator
for vanishing regularization:  
\[
\hbeta = \lim_{\lambda \to 0} \, (X^\T X + \lambda I)^{-1} X^\T Y =   
\lim_{\lambda \to 0} \, \argmin_\beta \; \|Y - X\beta\|_2^2 + \lambda 
\|\beta\|_2^2.   
\]
When $X$ has rank $d$, the min-norm least squares estimator reduces to
\smash{$\hbeta = (X^\T X)^{-1} X^\T Y$}, the usual least squares estimator.
Importantly, when $X$ has rank $n$, this estimator interpolates the training 
data: \smash{$y_i = x_i^\T \hbeta$}, for $i=1,\dots,n$.   

\subsection{Connection to gradient descent}

There is an interesting connection between gradient descent and the min-norm
least squares estimator. Initialize $\beta^{(0)} = 0$, and consider running
gradient descent on the least squares loss, yielding iterates
\[
\beta^{(k)} = \beta^{(k-1)} + t X^\T (y-X\beta^{(k-1)}), 
\quad k=1,2,3,\dots,
\]
where we take the step size $t>0$ to be small enough. Then gradient descent
converges to the min-norm least squares solution in \eqref{eq:ls_min}:
\[
\lim_{k \to \infty} \, \beta^{(k)} = \hbeta.
\]
Why? It's quite simple: the updates \smash{$\beta^{(k)}$}, $k=1,2,3,\dots$
always lie in the row space of $X$; hence their limit (guaranteed to exist for
small enough $t>0$) must also lie in the row space of $X$; and the min-norm
least squares solution is the unique least squares solution with this property.    

The same result (and proof) carries over to stochastic gradient descent, and any 
other variants of gradient descent whose updates remain in the row space of $X$.  
Since these algorithms comprise the defacto standard for training neural
networks, one can argue that that minimum $\ell_2$ norm solutions arise
naturally as interesting objects of study based on practical conventions (and
successes) in machine learning.  

\section{Problem setup}

\def\asto{\overset{\mathrm{as}}{\to}}
\def\dto{\overset{d}{\to}}
\def\Risk{\mathrm{Risk}}
\def\Bias{\mathrm{Bias}}
\def\hSigma{\hat\Sigma}

We will assume the usual linear model 
\begin{equation}
\label{eq:model}
Y = X\beta_0 + \epsilon,
\end{equation}
where $\epsilon \in \R^n$ has i.i.d.\ entries with mean zero and variance
$\sigma^2$, and $\epsilon \indep X$. The conditions we will assume on the
features $X \in \R^{n \times d}$ are as follows:
\begin{itemize}
\item[(A1)] $X = Z \Sigma^{1/2}$, where the entries of $Z \in \R^{n \times d}$
  are i.i.d.\ with mean zero, unit variance, and finite $8+\eta$ moment, for
  some $\eta>0$; 
\item[(A2)] the covariance matrix $\Sigma \in \R^{d \times d}$ has eigenvalues
  bounded away from $0$ and $\infty$, and satisfies \smash{$F_\Sigma \dto H$},
  as $n,d \to \infty$; 
\item[(A3)] $d/n \to \gamma \in (1,\infty)$ as $n,d \to \infty$.
\end{itemize}
To emphasize, we are considering here $\gamma > 1$, called the
\emph{overparametrized} regime. As in the ridge lecture, we will be interested
in the out-of-sample prediction risk, conditional on $X$, 
\begin{equation}
\label{eq:risk}
\Risk_X(\hbeta; \beta_0) = \E \big[ ( x_0^\T \hbeta - x_0^\T \beta_0 )^2
\,\big|\, X \big],
\end{equation}
where $x_0 = \Sigma^{1/2} z_0$ is i.i.d.\ to the rows of $X$. We will study the
risk of min-norm least squares in \eqref{eq:ls_min}. 

\subsection{Summary}

Here is a summary of what will happen in linear models, based roughly on the
development of results  in \citet{hastie2022surprises}. At the outset, we should
note that there are a lot of other interesting results from other recent papers
that we will not be covering. This includes some work that goes beyond linear
feature models and budges closer what actually happens in neural networks. For
nice surveys on the recent explosion of work on overparametrization theory, see 
\citet{bartlett2021deep, belkin2021fit, dar2021farewell}.      

\begin{enumerate}
\item[0.]
In the underparametrized regime ($\gamma < 1$), the risk is purely variance
(there is no bias), and does not depend on $\beta_0$ or $\Sigma$. Indeed, recall
that we already showed in the last lecture that
\begin{equation}
\label{eq:ls_risk_limit}
\Risk_X(\hbeta; \beta_0) \asto \sigma^2 \frac{\gamma}{1-\gamma}.
\end{equation}
This blows up as $\gamma \to 1$ from below. 

\item
In the overparametrized regime ($\gamma > 1$), the risk is composed of both
bias and variance, and generally depends on $\Sigma$ or $\beta_0$. The
asymptotic risk descends from its asymptote at $\gamma = 1$, but there is no
longer a simple explicit formula that describes its behavior with $\gamma$ in
full generality.

\item
In the isotropic case $\Sigma = I$, we can derive a simple formula for the
limiting risk for fixed $\beta_0$. This case already exhibits some interesting
and informative properties. For example, in a misspecified model (where the mean
$\E[y_i|x_i]$ is no longer linear in $x_i$ in \eqref{eq:model}), the risk can
attain its global minimum at $\gamma \in (1,\infty)$. 

However, this case fails to shed light on other important aspects that we seek
to understand; in particular, optimal regularization strength in ridge
regression in this isotropic case is always be positive.

\item
In the case of general $\Sigma$, we can derive an explicit formula for the
asymptotic variance. As in the ridge analysis, this is no longer closed-form,
but still we can learn various things. For example, in specific covariance
structures, we can study the variance term as a function of correlation
strength.    

The behavior of the bias is much more complex. With a prior on $\beta_0$, we can 
get back an explicit asymptotic formula (as in the ridge lecture), but this
again fails to shed light on important aspects we would like to study, as
optimal regularization strength in ridge regression is again always positive.     

\item
Thus to expose when and how ridgeless regularization ($\lambda \to 0$) can be
optimal, we must study the bias for general $\Sigma$, and fixed $\beta_0$. This
is a lot more challenging, but it can be done. The optimality of ridgeless 
regularization can rigorously confirmed in a latent space model (which is a kind
of misspecified model). 

\item
Finally, though still much more challenging, concrete progress can be made
outside of the linear feature model. In certain nonlinear settings, some aspects
of the lessons from linear feature models are preserved. You'll explore this 
(numerically) on the homework.  
\end{enumerate}

\subsection{Plan-of-attack?}

In general, there are two ways to proceed to derive the risk of the ridgeless
regression estimator \eqref{eq:ls_min}. The first is to take asymptotic results
for ridge regression (such as those we derived in the previous lecture) and then
let $\lambda \to 0$, which typically requires us to be careful about how to
resolve indeterminate limits. 

The second is to start with the bias and variance expressions for min-norm 
least squares, and then carry out asymptotic calculations directly. Often, these
calculations require us to ``ridge-ify'' some of the matrix functionals
that involve pseudoinverses, and then let the auxiliary ridge parameter tend to
zero. Curiously, the ``ridge-ified'' functionals in the second approach not
exactly the same as those that appear in the asymptotic calculations for ridge 
regression, and the former can sometimes be easier to analyze in the ridgeless
limit (of course we must end up with the same answers at the end with either
approach).  

We will generally follow the second plan-of-attack, if only to give you more
exposure to the elegant and powerful nature of random matrix theory. (Though to
be frank in what follows we will only really cover proof details for the
isotropic case.) In preparation for this, it is helpful to rewrite the ridgeless
estimator  \eqref{eq:ls_min} as \smash{$\hbeta = (X^\T X/n)^+ X^\T Y/n$}, and
helpful to record the bias and variance components of its risk:  
\begin{align}
\label{eq:ls_min_bias}
\Bias_X(\hbeta; \beta_0) &= \beta_0^\T (I - \hSigma^+ \hSigma) \Sigma 
  (I - \hSigma^+ \hSigma) \beta_0 \\  
\label{eq:ls_min_var}
\Var_X(\hbeta) &= \frac{\sigma^2}{n} \tr (\hSigma^+ \Sigma),
\end{align}
where \smash{$\hSigma = X^\T X/n$}, which follows from calculations similar to
those in the underparametrized case.      

\section{Isotropic $\Sigma$, fixed $\beta_0$}

We consider the isotropic case, $\Sigma = I$.  We will assume that $\|\beta_0\|_2
= r$ (which is a constant that does not vary with $n,d$), for the true signal
vector in \eqref{eq:model}. Below we analyze the bias and variance separately.

\paragraph{Bias analysis.}

When $\Sigma = I$, the bias \eqref{eq:ls_min_bias} becomes 
\begin{align}
\nonumber
\Bias_X(\hbeta; \beta_0) 
&= \beta_0^\T (I - \hSigma^+ \hSigma) \beta_0 \\ 
\nonumber
&= \lim_{z \to 0} \, \beta_0^\T \big[ I - (\hSigma + zI)^{-1} \hSigma \big]
  \beta_0 \\   
\label{eq:ls_min_bias_iso}
&= \lim_{z \to 0} \, z \beta_0^\T (\hSigma + zI)^{-1} \beta_0, 
\end{align}
where in the second line we used the general fact \smash{$(A^\T A)^+ A^\T = 
  \lim_{z \to 0} \, (A^\T A + zI)^{-1} A^\T$}, to \smash{$A = X/\sqrt{n}$}, and 
in the third line we added and subtracted $zI$ to trailing \smash{$\hSigma$} in
the matrix product. (All limits $z \to 0$ here and henceforth are to be
interpreted as from above.)

By the generalized MP theorem from \citet{rubio2011spectral} (transcribed in
Theorem 2 in the ridge lecture notes using the language of deterministic
equivalents), we know that  
\[
\beta_0^\T (\hSigma + zI)^{-1} \beta_0 \quad \text{and} \quad 
\beta_0^\T (a_n I + zI)^{-1} \beta_0 = \frac{r^2}{a_n + z} \quad
\text{have the same asymptotic limit},
\]
where $a_n$ satisfies a certain fixed-point equation. To compute the limit of
$a_n$, we note that also
\[
\frac{1}{d} \tr \big[ (\hSigma + zI)^{-1} \big] \quad \text{and} \quad 
\frac{1}{d} \tr \big[ (a_n I + zI)^{-1} \big] = \frac{1}{a_n + z}
\quad \text{have the same asymptotic limit},  
\]
and by the standard MP asymptotics, the left-hand side converges almost surely
to $m_F(-z)$, the Stieltjes transform of the MP law, evaluated at $-z$. Thus we
have $1/(a_n + z) \to m_F(-z)$, and   
\[
\beta_0^\T (\hSigma + zI)^{-1} \beta_0 \asto r^2 m_F(-z).
\]
Returning to \eqref{eq:ls_min_bias_iso}, some calculations involving the
Moore-Osgood theorem (whose details we omit) allow us to switch the order of the
limits as $n,d \to \infty$ and $z \to 0$, yielding
\begin{align}
\nonumber
\Bias_X(\hbeta; \beta_0) &\asto r^2 \lim_{z \to 0} \, z m_F(-z) \\
\nonumber
&= r^2 \lim_{z \to 0} \, \frac{-(1 - \gamma + z) + \sqrt{(1 - \gamma + z)^2 +
  4 \gamma z}}{2 \gamma} \\
\nonumber
&= r^2 \frac{-(1-\gamma) + (\gamma-1)}{2 \gamma} \\
\label{eq:ls_min_bias_iso_limit}
&= r^2 \Big( 1- \frac{1}{\gamma} \Big),
\end{align}
where to compute the limit we have used the explicit form of the Stieltjes
transform of the MP law (as covered in the ridge lecture notes).

\paragraph{Variance analysis.}

When $\Sigma = I$, the variance analysis is actually quite easy (relatively
speaking). From \eqref{eq:ls_min_var}, we have
\[
V_X(\hbeta; \beta_0) = \sigma^2 \sum_{i=1}^n \frac{1}{s_i(X^\T X)},
\]
where $s_i(X^\T X/n)$, $i=1,\dots,n$ denote the nonzero eigenvalues of $X^\T 
X/n$. Then because $X^\T X$ and $XX^\T$ always have the same nonzero
eigenvalues, we may write this as
\[
V_X(\hbeta; \beta_0) = \sigma^2 \sum_{i=1}^n \frac{1}{s_i(X^\T X)} = 
\frac{\sigma^2 n}{d} \frac{1}{n} \tr\big[ (XX^\T /d)^{-1} \big].
\]
The rightmost term above is precisely the form of the variance in the
underparametrized case, but with $X^\T$ in place of $X$. Thus, denoting $\tau =
n/d < 1$, we conclude from our previous analysis that  
\begin{equation}
\label{eq:ls_min_var_iso_limit}
V_X(\hbeta; \beta) \asto \sigma^2 \frac{\tau}{1-\tau} = \sigma^2 
\frac{1}{\gamma-1},  
\end{equation}

\paragraph{Putting it together.}

Adding the bias \eqref{eq:ls_min_bias_iso_limit} and variance results
\eqref{eq:ls_min_var_iso_limit} together, we get
\begin{equation}
\label{eq:ls_min_risk_iso_limit}
\Risk_X(\hbeta; \beta_0) \asto r^2 \Big( 1- \frac{1}{\gamma} \Big) + \sigma^2 
\frac{1}{\gamma-1}. 
\end{equation}
This has quite a striking profile:
\begin{quote}
\centering\it
The out-of-sample risk of least squares descends from its asymptote at the
interpolation threshold $\gamma = 1$. The bias grows with $\gamma$, but 
the variance decreases with $\gamma$.      
\end{quote}
The calculation above explains why the variance decreases with $\gamma$: we
found that the variance of min-norm least squares on $n$ samples and $d$
features is precisely the variance of \emph{ordinary} least squares on $d$
samples and $n$ features. See Figure \ref{fig:risk_iso} for visualization of the
risk profile over $\gamma$.

\begin{figure}[htb]
\centering
\includegraphics[width=0.6\textwidth]{risk_iso.pdf}
\caption{\it The asymptotic risk of least squares \eqref{eq:ls_risk_limit} for
  $\gamma<1$ and min-norm least squares \eqref{eq:ls_min_risk_iso_limit} for
  $\gamma>1$, where the latter assumes $\Sigma = I$, the isotropic
  case. Different colors represent different values of $\mathrm{SNR} = r^2 /
  \sigma^2$. The points denote finite-sample risks, computed from i.i.d.\
  standard Gaussian features with $n=200$ and $d=[\gamma n]$ for varying
  $\gamma$. Credit: \citet{hastie2022surprises}.}  
\label{fig:risk_iso}
\end{figure}

\section{Misspecified setting}

The latter analysis portrays an interesting feature of ridgeless regression in  
the overparametrized regime: its variance is well-controlled past the
interpolation threshold $\gamma = 1$. However, it fails to exhibit any real
benefit to doing ridgeless regression in the overparametrized regime, because
the global minimum of the risk is always at $\gamma = 0$. We also do not see
double descent in Figure \ref{fig:risk_iso}, as the first descent over $\gamma
\in (0,1)$ never happens. This is due to the fact that we are assuming, for any 
$n,d$, a true linear model in $d$ features, which makes it hard to reason about
what happens across different dimensions $d$.    

We therefore pursue a small but important change to the model in
\eqref{eq:model}: we instead assume that    
\begin{equation}
\label{eq:model_mis}
Y = X\beta_0 + W\theta_0 + \epsilon,
\end{equation}
Everything is the same as before, with respect to the distributions of $X$ and
$\epsilon$. But now we have an additional part of the mean function that is
driven by $W$, which we treat as a matrix of \emph{unobserved} features (that we 
do not have in hand). Thus we still just carry out regression of $Y$ on $X$, as
in \eqref{eq:ls_min}. In this context, we call \eqref{eq:model_mis} a
\emph{misspecified} model, and analogous to \eqref{eq:risk}, we are now
interested in risk defined as:
\begin{equation}
\label{eq:risk_mis}
\Risk_X(\hbeta; \beta_0, \theta_0) = \E \big[ \big( x_0^\T \hbeta -
\E[y_0|x_0,w_0] \big)^2 \,\big|\, X \big],
\end{equation}
where $(x_0,w_0,y_0)$ is an i.i.d.\ draw from the same joint distribution as in 
\eqref{eq:model_mis}. 

\subsection{Risk decomposition} 

We can always write 
\begin{equation}
\label{eq:risk_mis_decomp}
\Risk_X(\hbeta; \beta_0, \theta_0) = 
\underbrace{\E \big[ \big( x_0^\T \hbeta - \E[y_0|x_0] \big)^2 \,|\, X 
  \big]}_{L_X(\hbeta; \beta_0, \theta_0)} +
\underbrace{\E \big[ \big( \E[y_0|x_0] - \E[y_0|x_0,w_0] \big)^2
  \big]}_{M(\beta_0, \theta_0)}, 
\end{equation}
which is verified by simply verified by first conditioning on $x_0$, then adding
an subtracting $\E[y_0|x_0]$ inside the square in the definition of
\smash{$R_X(\hbeta; \beta_0, \theta_0)$} in \eqref{eq:risk_mis}, and expanding, 
and noting that the cross term is zero:   
\[
\E \big[ \big( x_0^\T \hbeta - \E[y_0|x_0] \big) \,|\, X,x_0 \big]
\E \big[\big( \E[y_0|x_0] - \E[y_0|x_0,w_0] \big) \,|\, x_0 \big] = 0.
\]

In other words, from \eqref{eq:risk_mis_decomp}, we learn that the risk in the
misspecified setting decomposes into two terms: \smash{$L_X(\hbeta; \beta_0,
  \theta_0)$}, measuring how well we can predict the conditional mean of
$\E[y_0|x_0]$, and \smash{$M_X(\hbeta; \beta_0, \theta_0)$}, measuring how far
apart $\E[y_0|x_0]$ and $\E[y_0|x_0,w_0]$ are. We call the latter term the
\emph{misspecification bias} (note that it does not depend at all on $X$ or
\smash{$\hbeta$}).   

\subsection{Simplest analysis}

In the simplest case, we can take the distribution of the unobserved features
$W$ to be independent of $X$ (and $\epsilon$), with the covariances of $X,W$
each being the identity---in keeping with the isotropic setting just
studied. This allows to rewrite \eqref{eq:risk_mis} as 
\[
Y = X\beta_0 + \delta,
\]
where the entries of $\delta = W\theta_0 + \epsilon$ are still i.i.d.\ with mean
zero, and their variance is $\|\theta_0\|_2^2 + \sigma^2$. Denote 
\[
r^2=\|\beta_0\|_2^2+\|\theta_0\|_2^2 \quad \text{and} \quad 
\kappa=\|\beta_0\|_2^2 / r^2,
\]
which represents the total signal energy and the the fraction of the signal
energy captured by the observed features, respectively. Then \smash{$L_X(\hbeta;
  \beta_0, \theta_0)$} behaves exactly as we computed previously, in
\eqref{eq:ls_risk_limit} for $\gamma<1$ and \eqref{eq:ls_min_risk_iso_limit} for
$\gamma>1$, after we make the substitutions:     
\[
r^2 \mapsto r^2 \kappa \quad \text{and} \quad
\sigma^2 \mapsto \sigma^2 + r^2 (1-\kappa).
\]
Furthermore, we can easily calculate the misspecification bias as: 
\[
M(\beta_0, \theta_0) = \E[(w_0^\T \theta_0)^2] = r^2 (1-\kappa). 
\]
Putting these results together leads to the following conclusion:
\begin{equation}
\label{eq:ls_min_risk_iso_mis_limit}
\Risk_X(\hbeta; \beta_0; \theta_0) \asto 
\begin{cases}
r^2(1-\kappa) + \big(r^2(1-\kappa) + \sigma^2\big) 
\frac{\gamma}{1-\gamma} & \text{for $\gamma < 1$}, \\
r^2(1-\kappa) + r^2 \kappa \big(1-\frac{1}{\gamma}\big) 
+ \big(r^2(1-\kappa) + \sigma^2\big) \frac{1}{\gamma-1} &  
\text{for $\gamma > 1$}.
\end{cases}
\end{equation}

\subsection{Interpretation}

To interpret the risk profiles \eqref{eq:ls_min_risk_iso_mis_limit} in the
misspecified setting, we will need to specify a relationship between $\kappa$
and $\gamma$. Since adding features should generally improve our approximation
capacity, it is reasonable to model $\kappa=\kappa(\gamma)$ as an increasing 
function of $\gamma$. Figure \ref{fig:risk_iso_mis} gives an example with a
polynomial decay, $1-\kappa(\gamma) = (1+\gamma)^{-a}$. Notably, we can see a
clear double descent in the risk curve, and for certain values of $a$ (such as
that plotted in green), we find that the global min of the risk occurs at
$\gamma > 1$.  

\begin{figure}[htb]
\centering
\includegraphics[width=0.6\textwidth]{risk_iso_mis.pdf}
\caption{\it The asymptotic risk of min-norm least squares in the misspecified
  setting \eqref{eq:ls_min_risk_iso_mis_limit}, with isotropic observed and
  unobserved feature covariances, and $1-\kappa(\gamma) = (1+\gamma)^{-a}$. We
  set $\mathrm{SNR} = r^2 / \sigma^2 = 5$. Different colors represent different
  values of $a$. As in Figure \ref{fig:risk_iso}, the points denote
  finite-sample risks, computed from $n=200$ and $d=[\gamma n]$ for varying 
  $\gamma$. Credit: \citet{hastie2022surprises}.}   
\label{fig:risk_iso_mis}
\end{figure}

As a concluding remark in this misspecified setting, we note that the dimension
of the unobserved features (the number of columns of $W$) enters nowhere in
these calculations, so we can effectively think of it as infinite. This provides
us with a nice interpretation: we have an infinitely wide matrix $[X; W]$ that 
governs the behavior of $Y$ in the model \eqref{eq:model}. As $d$ grows, we
observe more and more columns of this matrix, which improves our approximation
capacity. This is an analogy to what a feature generator can do for us.       

\section{General $\Sigma$, random $\beta_0$}

In the general $\Sigma$ case, just as in the ridge analysis, the bias in 
\eqref{eq:ls_min_bias} is especially difficult to calculate. We can make
progress by placing a spherical prior on $\beta_0$, such that        
\begin{equation}
\label{eq:prior}
\E[\beta_0 \beta_0^\T] = \frac{r^2}{d} I.
\end{equation}

\paragraph{Bias analysis.}

By similar arguments to those in the isotropic case above and the ridge lecture,
whose details we omit, one can show that for the Bayes bias, given by
integrating \eqref{eq:ls_min_bias} over $\beta_0$,
\begin{equation}
\label{eq:ls_min_bias_bayes_limit}
\Bias_X(\hbeta) \asto \frac{r^2}{\gamma} \frac{1}{v_F(0)},  
\end{equation}
where $v_F$ is the companion Stieltjes transform of the limiting spectral
distribution $F = F(H, \gamma)$ from the Marchenko-Pastur theorem (and
\smash{$v_F(0) = \lim_{z \to 0} \, v_F(z)$} exists under our assumptions).  

\paragraph{Variance analysis.}

Again by similar arguments to those in the isotropic case and the ridge lecture,
whose details we also omit, one can show that for the variance in
\eqref{eq:ls_min_var},  
\begin{equation}
\label{eq:ls_min_var_limit}
\Var_X(\hbeta) \asto \sigma^2 \bigg(\frac{v'_F(0)}{v_F(0)^2} - 1 \bigg),
\end{equation}
where again $v_F$ is the companion Stieltjes transform of the limiting spectral 
distribution $F = F(H, \gamma)$ from the MP theorem (and
\smash{$v'_F(0)/v_F(0)^2 = \lim_{z \to 0} \, v'_F(z)/v_F(z)^2$} exists under our
assumptions). We emphasize that the variance calculation in
\eqref{eq:ls_min_var_limit} does not depend on the prior \eqref{eq:prior} and is
fully general.    

\paragraph{Investigation of asymptotics.}

We note that the some of the key dependence of
\eqref{eq:ls_min_bias_bayes_limit}, \eqref{eq:ls_min_var_limit} on $\gamma$ is
hidden in the Stieltjes transform terms $v_F(0)$, $v'_F(0)$, which themselves
depends on $\gamma$ (since $F$ does). While these asymptotic limit cannot be
computed in closed-form for general covariance models (general $H$), they can be
computed numerically by solving the Silverstein equation. This is done in
\citet{hastie2022surprises}, in order to probe the asymptotic limits, and better
understand how they behave. This reveals some interesting phenomenon, such as
the fact that stronger correlations can increase the variance, but decrease the
Bayes bias. See Figure \ref{fig:risk_ec_ar}. 

\begin{figure}[p]
\centering
\includegraphics[width=0.49\textwidth]{bias_ec.pdf}
\includegraphics[width=0.49\textwidth]{var_ec.pdf}
\includegraphics[width=0.49\textwidth]{bias_ar.pdf}
\includegraphics[width=0.49\textwidth]{var_ar.pdf}
\caption{\it Asymptotic Bayes bias and variance of min-norm least squares for
  different feature covariance models. The top row shows an equicorrelated
  feature model, where $\Sigma_{ij} = 1$ if $i=j$ and $\rho$ otherwise. In this 
  model the asymptotics can be done in closed-form: we find the bias decreases
  with $\rho$, whereas the variance does not actually depend on $\rho$. The
  bottom row shows an autocorrelated feature model, where $\Sigma_{ij} = 
  \rho^{|i-j|}$. Here the asymptotics cannot be done in closed-form but they can  
  be efficiently computed numerically: we find that the bias decreases with
  $\rho$, and the variance increases with $\rho$. Credit:
  \citet{hastie2022surprises}.}      
\label{fig:risk_ec_ar}
\end{figure}

\paragraph{Where this fall short.}

The Bayes model studied here falls short in one key way. It does not explain why
ridgeless regression is statistically interesting above and beyond ridge
regression. In the Bayes model studied here, recall (from the last lecture), we
know that the asymptotically optimal ridge tuning parameter is $\lambda^* =
\sigma^2 \gamma / r^2$, regardless of the feature covariance model. This is
always positive, and depends only on the SNR. Thus in order to really understand
the phenomena in Figure \ref{fig:experiments}, i.e., to understand when and how
the ridge risk landscape can be minimized at $\lambda = 0$, we must tackle the
beast: calculate the ridge bias for general $\Sigma$ and fixed $\beta_0$.     

\section{General $\Sigma$, fixed $\beta_0$}

Todo. Following Sections 4 and 6 of \citet{hastie2022surprises}.

\bibliographystyle{plainnat}
\bibliography{../../common/ryantibs}

\end{document}
